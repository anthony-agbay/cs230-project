{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search\n",
    "\n",
    "Before evaluating a final model, we will first go through a hyperparamter search for the models. To reduce computational waste, we will only be training and evaluating hyperparamters using the LOPO strategy on Kka2, a protein with a relatively small number of samples but a good balance of labels. The hyperparamters will we be selecting for are:\n",
    "\n",
    "## Hyperparameters\n",
    "- Number of hidden layers\n",
    "- Number of nodes in each hidden layer\n",
    "- Labeling Thresholds\n",
    "\n",
    "Attempts at normalization and regularization didn't work with our current data in initial baseline runs, so they have been exlcuded from this analysis.\n",
    "\n",
    "## Analysis Criterion\n",
    "To choose our final hyperparameters, we will identify the model with the best precision and accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import optimizers, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in Data\n",
    "data_path = 'data/merged.csv'\n",
    "data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation functions\n",
    "\n",
    "def label_type(row, thresh_offset):\n",
    "    \"\"\" Converts continuous label to categorical label\n",
    "    \"\"\"\n",
    "    if row['scaled_effect'] < 1 - thresh_offset:\n",
    "        return('Deleterious')\n",
    "    elif row['scaled_effect'] > 1 + thresh_offset:\n",
    "        return('Beneficial')\n",
    "    else:\n",
    "        return('Netural')\n",
    "    \n",
    "def lopo_train_test_split(protein, curr_data):\n",
    "    train_data = curr_data[data.protein != protein].drop(['protein', 'pdb', 'resnum'], axis=1)\n",
    "    test_data = curr_data[data.protein == protein].drop(['protein', 'pdb', 'resnum'], axis=1)\n",
    "    \n",
    "    \n",
    "    y_train = train_data.type\n",
    "    encoder_train = LabelEncoder()\n",
    "    encoder_train.fit(y_train)\n",
    "    y_train = to_categorical(encoder_train.transform(y_train))\n",
    "    \n",
    "    x_train = train_data.drop(['type'], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    y_test = test_data.type\n",
    "    encoder_test = LabelEncoder()\n",
    "    encoder_test.fit(y_test)\n",
    "    y_test = to_categorical(encoder_test.transform(y_test))\n",
    "    \n",
    "    x_test = test_data.drop(['type'], axis=1)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definitions\n",
    "def nn_model(num_layers, num_nodes):\n",
    "    model = Sequential()\n",
    "    inputs = Input(shape=(968,))\n",
    "    x = Dense(num_nodes, activation=tf.nn.relu)(inputs)\n",
    "    for layers in range(num_layers-1):\n",
    "        x = Dense(num_nodes, activation=tf.nn.relu)(inputs)\n",
    "    outputs = Dense(3, activation=tf.nn.softmax)(x)\n",
    "    opt = optimizers.Adam(learning_rate = 0.1)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy(),\n",
    "                           tf.keras.metrics.Precision(),\n",
    "                           tf.keras.metrics.Recall()])\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Model: HL - 2 | Nodes - 25 | Threshold Offset - 0.025\n",
      "Train on 48217 samples\n",
      "Epoch 1/50\n",
      "48217/48217 [==============================] - 31s 641us/sample - loss: 3.8274 - categorical_accuracy: 0.5731 - precision_14: 0.5730 - recall_14: 0.5332\n",
      "Epoch 2/50\n",
      "48217/48217 [==============================] - 30s 616us/sample - loss: 0.9844 - categorical_accuracy: 0.5738 - precision_14: 0.5728 - recall_14: 0.5203\n",
      "Epoch 3/50\n",
      "48217/48217 [==============================] - 30s 622us/sample - loss: 0.9849 - categorical_accuracy: 0.5738 - precision_14: 0.5743 - recall_14: 0.5200\n",
      "Epoch 4/50\n",
      "48217/48217 [==============================] - 30s 630us/sample - loss: 0.9843 - categorical_accuracy: 0.5738 - precision_14: 0.5723 - recall_14: 0.5203\n",
      "Epoch 5/50\n",
      "48217/48217 [==============================] - 30s 623us/sample - loss: 0.9841 - categorical_accuracy: 0.5733 - precision_14: 0.5731 - recall_14: 0.5317\n",
      "Epoch 6/50\n",
      "48217/48217 [==============================] - 30s 627us/sample - loss: 0.9839 - categorical_accuracy: 0.5738 - precision_14: 0.5737 - recall_14: 0.5221\n",
      "Epoch 7/50\n",
      "48217/48217 [==============================] - 30s 620us/sample - loss: 0.9842 - categorical_accuracy: 0.5738 - precision_14: 0.5735 - recall_14: 0.5281\n",
      "Epoch 8/50\n",
      "48217/48217 [==============================] - 30s 631us/sample - loss: 0.9851 - categorical_accuracy: 0.5737 - precision_14: 0.5730 - recall_14: 0.5239\n",
      "Epoch 9/50\n",
      "48217/48217 [==============================] - 30s 624us/sample - loss: 0.9848 - categorical_accuracy: 0.5738 - precision_14: 0.5734 - recall_14: 0.5227\n",
      "Epoch 10/50\n",
      "48217/48217 [==============================] - 31s 633us/sample - loss: 0.9844 - categorical_accuracy: 0.5738 - precision_14: 0.5736 - recall_14: 0.5255\n",
      "Epoch 11/50\n",
      "48217/48217 [==============================] - 30s 613us/sample - loss: 0.9847 - categorical_accuracy: 0.5734 - precision_14: 0.5742 - recall_14: 0.5100\n",
      "Epoch 12/50\n",
      "48217/48217 [==============================] - 30s 626us/sample - loss: 0.9842 - categorical_accuracy: 0.5738 - precision_14: 0.5741 - recall_14: 0.5252\n",
      "Epoch 13/50\n",
      "48217/48217 [==============================] - 30s 617us/sample - loss: 0.9854 - categorical_accuracy: 0.5738 - precision_14: 0.5728 - recall_14: 0.5271\n",
      "Epoch 14/50\n",
      "48217/48217 [==============================] - 30s 625us/sample - loss: 0.9842 - categorical_accuracy: 0.5738 - precision_14: 0.5737 - recall_14: 0.5182\n",
      "Epoch 15/50\n",
      "48217/48217 [==============================] - 30s 616us/sample - loss: 0.9846 - categorical_accuracy: 0.5738 - precision_14: 0.5745 - recall_14: 0.5191\n",
      "Epoch 16/50\n",
      "48217/48217 [==============================] - 30s 624us/sample - loss: 0.9848 - categorical_accuracy: 0.5738 - precision_14: 0.5737 - recall_14: 0.5113\n",
      "Epoch 17/50\n",
      "48217/48217 [==============================] - 29s 605us/sample - loss: 0.9839 - categorical_accuracy: 0.5735 - precision_14: 0.5742 - recall_14: 0.5216\n",
      "Epoch 18/50\n",
      "48217/48217 [==============================] - 30s 624us/sample - loss: 0.9839 - categorical_accuracy: 0.5738 - precision_14: 0.5742 - recall_14: 0.5257\n",
      "Epoch 19/50\n",
      "48217/48217 [==============================] - 30s 618us/sample - loss: 0.9838 - categorical_accuracy: 0.5738 - precision_14: 0.5734 - recall_14: 0.5300\n",
      "Epoch 20/50\n",
      "48217/48217 [==============================] - 30s 619us/sample - loss: 0.9845 - categorical_accuracy: 0.5738 - precision_14: 0.5730 - recall_14: 0.5192\n",
      "Epoch 21/50\n",
      "48217/48217 [==============================] - 30s 618us/sample - loss: 0.9849 - categorical_accuracy: 0.5738 - precision_14: 0.5731 - recall_14: 0.5337\n",
      "Epoch 22/50\n",
      "48217/48217 [==============================] - 30s 625us/sample - loss: 0.9847 - categorical_accuracy: 0.5738 - precision_14: 0.5732 - recall_14: 0.5229\n",
      "Epoch 23/50\n",
      "48217/48217 [==============================] - 30s 616us/sample - loss: 0.9854 - categorical_accuracy: 0.5738 - precision_14: 0.5730 - recall_14: 0.5252\n",
      "Epoch 24/50\n",
      "48217/48217 [==============================] - 30s 615us/sample - loss: 0.9840 - categorical_accuracy: 0.5738 - precision_14: 0.5747 - recall_14: 0.5238\n",
      "Epoch 25/50\n",
      "48217/48217 [==============================] - 29s 608us/sample - loss: 0.9855 - categorical_accuracy: 0.5738 - precision_14: 0.5727 - recall_14: 0.5175\n",
      "Epoch 26/50\n",
      "48217/48217 [==============================] - 30s 621us/sample - loss: 0.9839 - categorical_accuracy: 0.5738 - precision_14: 0.5739 - recall_14: 0.5199\n",
      "Epoch 27/50\n",
      "48217/48217 [==============================] - 30s 617us/sample - loss: 0.9842 - categorical_accuracy: 0.5738 - precision_14: 0.5737 - recall_14: 0.5248\n",
      "Epoch 28/50\n",
      "48217/48217 [==============================] - 30s 625us/sample - loss: 0.9838 - categorical_accuracy: 0.5733 - precision_14: 0.5750 - recall_14: 0.5257\n",
      "Epoch 29/50\n",
      "48217/48217 [==============================] - 30s 612us/sample - loss: 0.9848 - categorical_accuracy: 0.5732 - precision_14: 0.5743 - recall_14: 0.5201\n",
      "Epoch 30/50\n",
      "48217/48217 [==============================] - 30s 615us/sample - loss: 0.9846 - categorical_accuracy: 0.5738 - precision_14: 0.5731 - recall_14: 0.5219\n",
      "Epoch 31/50\n",
      "48217/48217 [==============================] - 30s 613us/sample - loss: 0.9852 - categorical_accuracy: 0.5738 - precision_14: 0.5736 - recall_14: 0.5247\n",
      "Epoch 32/50\n",
      "23210/48217 [=============>................] - ETA: 15s - loss: 0.9846 - categorical_accuracy: 0.5732 - precision_14: 0.5744 - recall_14: 0.5207"
     ]
    }
   ],
   "source": [
    "# Hyperparamter List\n",
    "num_hl = [2, 4, 8, 16, 32]\n",
    "num_nodes = [25, 100, 400, 800]\n",
    "label_thresholds = [.025, .05, .1, .2]\n",
    "\n",
    "# Other Variables to define\n",
    "protein = 'Uba1'\n",
    "column_list = ['Hidden Layers', 'Number Nodes', 'Threshold Offset', 'Loss', 'Accuracy', 'Precision', 'Recall']\n",
    "\n",
    "# Evaluation Metric Storage\n",
    "eval_metrics = pd.DataFrame(columns=column_list)\n",
    "\n",
    "for hl in num_hl:\n",
    "    for nodes in num_nodes:\n",
    "        for thresholds in label_thresholds:\n",
    "            # Generate the Split Training Set\n",
    "            data['type'] = data.apply(lambda row: label_type(row, thresholds), axis = 1)\n",
    "            data_final = data.drop(['scaled_effect'], axis=1)\n",
    "            x_train, y_train, x_test, y_test = lopo_train_test_split(protein, data_final)\n",
    "            \n",
    "            # Build the Model\n",
    "            print(\"Current Model: HL - {} | Nodes - {} | Threshold Offset - {}\".format(hl, nodes, thresholds))\n",
    "            curr_model = nn_model(hl, nodes)\n",
    "            curr_model.fit(x_train, y_train, epochs = 50, batch_size = 10, verbose=1)\n",
    "            \n",
    "            # Calculate Evaluation Metrics\n",
    "            loss, acc, prec, rec = curr_model.evaluate(x_test, y_test)\n",
    "#             probs = curr_model.predict_proba(x_test)\n",
    "#             predictions = curr_model.predict(x_test)\n",
    "#             precision = precision_score(y_test, predictions, average=\"macro\", zero_division = 0)\n",
    "#             score = curr_model.score(x_test, y_test)\n",
    "            \n",
    "            # Append to Eval Storage\n",
    "            eval_metrics = eval_metrics.append(pd.DataFrame([[hl, nodes, thresholds, loss, acc, prec, rec]], columns=column_list))\n",
    "            \n",
    "eval_metrics.to_csv('hyperparamter-eval.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow2_p36)",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
